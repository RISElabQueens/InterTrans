{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import intertrans.protos_pb2 as ptpb\n",
    "from intertrans.utils import submit_request_cak, launch_inference_endpoints, stop_inference_endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Pipeline for Direct Translation CA@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unit_tests_request(subset_df, fullset_df, template_name, extraction_name, model_name, base_filename, base_path, server_url):\n",
    "    batch_request = ptpb.BatchTranslationRequest()\n",
    "    batch_request.file_base_name = base_filename\n",
    "    batch_request.file_save_path = base_path\n",
    "\n",
    "    for index, row in subset_df.iterrows():\n",
    "        request = ptpb.TranslationRequest()\n",
    "        request.id = str(index)\n",
    "        request.seed_language = row['source_lang']\n",
    "        request.target_language = row['target_lang']\n",
    "        request.seed_code = row['input_code']\n",
    "        request.model_name = model_name\n",
    "        request.used_languages.append(\"Go\")\n",
    "        request.used_languages.append(\"Java\")\n",
    "        request.used_languages.append(\"Python\")\n",
    "        request.used_languages.append(\"C++\")\n",
    "        request.used_languages.append(\"JavaScript\")\n",
    "        request.used_languages.append(\"Rust\")\n",
    "\n",
    "\n",
    "        request.prompt_template_name = template_name\n",
    "        request.regex_template_name = extraction_name\n",
    "\n",
    "        #We attach the test cases to the request\n",
    "        retrieved = fullset_df[(fullset_df.id == row.id) & ((fullset_df.source_lang == row.source_lang) | (fullset_df.target_lang == row.source_lang) & (fullset_df.source_lang == row.target_lang))]\n",
    "        assert retrieved.shape[0] != 0\n",
    "        assert retrieved.shape[0] <= 6\n",
    "\n",
    "        for i, r in retrieved.iterrows():\n",
    "            #We attach the test case for evaluation. This is not used in the prompt.\n",
    "            unittest = ptpb.UnitTestCase()\n",
    "            unittest.language = r['target_lang']\n",
    "            unittest.test_case = r['test_code']\n",
    "\n",
    "            request.test_suite.unit_test_suite.append(unittest)\n",
    "\n",
    "            #The prompt for HumanEval-X leaks the target signature name so the generated code matches the function name expected by the test case\n",
    "            signature = ptpb.TargetSignature()\n",
    "            signature.language = r['target_lang']\n",
    "            signature.signature = r['target_signature']\n",
    "            request.target_signatures.append(signature)\n",
    "\n",
    "        batch_request.translation_requests.append(request)\n",
    "\n",
    "    return batch_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fuzzy_tests_request(subset_df, template_name, extraction_name, model_name, base_filename, base_path, server_url):\n",
    "    batch_request = ptpb.BatchTranslationRequest()\n",
    "    batch_request.file_base_name = base_filename\n",
    "    batch_request.file_save_path = base_path\n",
    "\n",
    "    for index, row in subset_df.iterrows():\n",
    "        request = ptpb.TranslationRequest()\n",
    "        request.id = str(index)\n",
    "        request.seed_language = row['source_lang']\n",
    "        request.target_language = row['target_lang']\n",
    "        request.seed_code = row['input_code']\n",
    "        request.model_name = model_name\n",
    "\n",
    "        request.used_languages.append(\"Go\")\n",
    "        request.used_languages.append(\"Java\")\n",
    "        request.used_languages.append(\"Python\")\n",
    "        request.used_languages.append(\"C++\")\n",
    "        request.used_languages.append(\"JavaScript\")\n",
    "        request.used_languages.append(\"Rust\")\n",
    "\n",
    "        request.prompt_template_name = template_name\n",
    "        request.regex_template_name = extraction_name\n",
    "\n",
    "        fuzzytest1 = ptpb.FuzzyTestCase()\n",
    "        fuzzytest1.stdin_input = row['stdin_input_1']\n",
    "        fuzzytest1.expected_output = row['expected_output_1']\n",
    "\n",
    "        fuzzytest2 = ptpb.FuzzyTestCase()\n",
    "        fuzzytest2.stdin_input = row['stdin_input_2']\n",
    "        fuzzytest2.expected_output = row['expected_output_2']\n",
    "\n",
    "        fuzzytest3 = ptpb.FuzzyTestCase()\n",
    "        fuzzytest3.stdin_input = row['stdin_input_3']\n",
    "        fuzzytest3.expected_output = row['expected_output_3']\n",
    "\n",
    "        request.test_suite.fuzzy_suite.append(fuzzytest1)\n",
    "        request.test_suite.fuzzy_suite.append(fuzzytest2)\n",
    "        request.test_suite.fuzzy_suite.append(fuzzytest3)\n",
    "\n",
    "        batch_request.translation_requests.append(request)\n",
    "\n",
    "    return batch_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_unit_test_experiment(args_dict):\n",
    "    launch_ids = launch_inference_endpoints(args_dict['model_name'], args_dict['server_url'])\n",
    "    request = build_unit_tests_request(**args_dict)\n",
    "    submit_request_cak(request, args_dict['server_url'])\n",
    "    stop_inference_endpoints(launch_ids, args_dict['server_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_fuzzy_test_experiment(args_dict):\n",
    "    launch_ids = launch_inference_endpoints(args_dict['model_name'], args_dict['server_url'])\n",
    "    request = build_fuzzy_tests_request(**args_dict)\n",
    "    submit_request_cak(request, args_dict['server_url'])\n",
    "    stop_inference_endpoints(launch_ids, args_dict['server_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder_dataset_all = pd.read_json('../datasets/transcoder_dataset_all.jsonl', orient='records', lines=True)\n",
    "humanevalx_all = pd.read_json('../datasets/humanevalx_dataset_all.jsonl', orient='records', lines=True)\n",
    "humanevalx_subset = pd.read_json('../datasets/humanevalx_dataset_subset.jsonl', orient='records', lines=True)\n",
    "codenet_subset = pd.read_json('../datasets/codenet_dataset_subset.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Run outside of Jupyter\n",
    "#cd ../engine && go run intertrans.go runserver ../paper/notebooks/configs/config_transcoder_noverify_ca10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransCoder Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CodeLlama 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'subset_df': transcoder_dataset_all,\n",
    "    'fullset_df': transcoder_dataset_all,\n",
    "    'template_name': 'prompt_transcoder',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    'base_filename': 'codellama_13b_transcoder_results_all_depth4_ca10',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_unit_test_experiment(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magicoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'subset_df': transcoder_dataset_all,\n",
    "    'fullset_df': transcoder_dataset_all,\n",
    "    'template_name': 'prompt_transcoder',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"ise-uiuc/Magicoder-S-DS-6.7B\",\n",
    "    'base_filename': 'magicoder_transcoder_results_all_depth4_ca10',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_unit_test_experiment(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StarCoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'subset_df': transcoder_dataset_all,\n",
    "    'fullset_df': transcoder_dataset_all,\n",
    "    'template_name': 'prompt_transcoder',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"bigcode/starcoder2-15b-instruct-v0.1\",\n",
    "    'base_filename': 'starcoder2_transcoder_results_all_depth4_ca10',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_unit_test_experiment(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HumanEval-X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run outside of Jupyter\n",
    "#cd ../engine && go run intertrans.go runserver ../paper/notebooks/configs/config_codenet_humanevalx_noverify_ca10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code LLaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'subset_df': humanevalx_subset,\n",
    "    'fullset_df': humanevalx_all,\n",
    "    'template_name': 'prompt_humanevalx',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    'base_filename': 'codellama_13b_humanevalx_results_sub_depth4_ca85',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_unit_test_experiment(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magicoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'subset_df': humanevalx_subset,\n",
    "    'fullset_df': humanevalx_all,\n",
    "    'template_name': 'prompt_humanevalx',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"ise-uiuc/Magicoder-S-DS-6.7B\",\n",
    "    'base_filename': 'magicoder_humanevalx_results_sub_depth4_ca85',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_unit_test_experiment(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StarCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'subset_df': humanevalx_subset,\n",
    "    'fullset_df': humanevalx_all,\n",
    "    'template_name': 'prompt_humanevalx',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"bigcode/starcoder2-15b-instruct-v0.1\",\n",
    "    'base_filename': 'starcoder2_humanevalx_results_sub_depth4_ca85',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_unit_test_experiment(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CodeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CodeLlama 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'subset_df': codenet_subset,\n",
    "    'template_name': 'prompt_codenet',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    'base_filename': 'codellama_13b_codenet_results_sub_depth4_ca10',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_fuzzy_test_experiment(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'subset_df': codenet_subset,\n",
    "    'template_name': 'prompt_codenet',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"ise-uiuc/Magicoder-S-DS-6.7B\",\n",
    "    'base_filename': 'magicoder_codenet_results_sub_depth4_ca10',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_fuzzy_test_experiment(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping for 60 seconds to allow the endpoints to be ready\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubset_df\u001b[39m\u001b[38;5;124m'\u001b[39m: codenet_subset,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemplate_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_codenet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver_url\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:50051\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m \u001b[43mexecute_fuzzy_test_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mexecute_fuzzy_test_experiment\u001b[0;34m(args_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m launch_ids \u001b[38;5;241m=\u001b[39m launch_inference_endpoints(args_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m], args_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver_url\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m request \u001b[38;5;241m=\u001b[39m build_fuzzy_tests_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_dict)\n\u001b[0;32m----> 4\u001b[0m \u001b[43msubmit_request_cak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mserver_url\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m stop_inference_endpoints(launch_ids, args_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver_url\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/home/intertrans/engine/client/intertrans/utils.py:26\u001b[0m, in \u001b[0;36msubmit_request_cak\u001b[0;34m(batch_request, grpc_channel_address)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39minsecure_channel(grpc_channel_address, options\u001b[38;5;241m=\u001b[39moptions) \u001b[38;5;28;01mas\u001b[39;00m channel:\n\u001b[1;32m     25\u001b[0m     stub \u001b[38;5;241m=\u001b[39m ptgrpc\u001b[38;5;241m.\u001b[39mTranslationServiceStub(channel)\n\u001b[0;32m---> 26\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchTranslateCAK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.envs/envs/expandclient/lib/python3.9/site-packages/grpc/_channel.py:1178\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1175\u001b[0m     (\n\u001b[1;32m   1176\u001b[0m         state,\n\u001b[1;32m   1177\u001b[0m         call,\n\u001b[0;32m-> 1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.envs/envs/expandclient/lib/python3.9/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:400\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args_dict = {\n",
    "    'subset_df': codenet_subset,\n",
    "    'template_name': 'prompt_codenet',\n",
    "    'extraction_name': 'temperature',\n",
    "    'model_name': \"bigcode/starcoder2-15b-instruct-v0.1\",\n",
    "    'base_filename': 'starcoder2_codenet_results_sub_depth4_ca10',\n",
    "    'base_path': '../data/raw_outputs/engine/noverify',\n",
    "    'server_url': 'localhost:50051'\n",
    "}\n",
    "\n",
    "execute_fuzzy_test_experiment(args_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (expandclient)",
   "language": "python",
   "name": "expandclient"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
